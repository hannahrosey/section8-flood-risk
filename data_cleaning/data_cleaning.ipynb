{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3b609f",
   "metadata": {},
   "source": [
    "## Advanced GIS: Interactive Web Mapping\n",
    "#### Final Project | 3/31/2022\n",
    "**Purpose**: clean and combine housing choice voucher data and neighborhood tabulation geographies for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780d7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and custom functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import geojson\n",
    "import geopandas as gpd\n",
    "import requests as r\n",
    "from tabula.io import read_pdf\n",
    "import tabula\n",
    "\n",
    "def get_county(x):\n",
    "    c = re.findall('NY New York [\\d]{3} (.* County)',x)\n",
    "    if len(c) > 0:\n",
    "        return(c[0])\n",
    "    else:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0798700",
   "metadata": {},
   "source": [
    "### Read in voucher data; add fields for filtering, joining, and analysis; filter to just NYC; handle missing data\n",
    "\n",
    "**Source**: https://www.huduser.gov/portal/datasets/assthsg.html#2009-2021_data, 2021 data\n",
    "\n",
    "**Documentation**: https://www.huduser.gov/portal/datasets/pictures/dictionary_2021.pdf\n",
    "\n",
    "**Definition of Missing values**\n",
    "Some cell entries across variables report no data or are suppressed. In such cases\n",
    "one of the following codes will apply to such missing values in the downloadable file\n",
    "\"NA\" = Not applicable\n",
    "\"-1\" = Missing\n",
    "\"-4\" = Suppressed (where the cell entry is less than 11 for reported families)\n",
    "\"-5\" = Non-reporting (where reporting rates--see % Reported--are less than 50%) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905668f",
   "metadata": {},
   "source": [
    "* The Bronx is Bronx County (ANSI / FIPS 36005)\n",
    "* Brooklyn is Kings County (ANSI / FIPS 36047)\n",
    "* Manhattan is New York County (ANSI / FIPS 36061)\n",
    "* Queens is Queens County (ANSI / FIPS 36081)\n",
    "* Staten Island is Richmond County (ANSI / FIPS 36085)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11095de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in voucher data\n",
    "dat = pd.read_excel(\"TRACT_MO_WY_2021.xlsx\")\n",
    "\n",
    "## Filter to NY State\n",
    "dat = dat.loc[dat.states == \"NY New York\"]\n",
    "\n",
    "## Check that entire dataset is at census tract level\n",
    "assert len(dat.loc[dat.sumlevel!=7]) == 0, \"Error! Mixed levels of analysis; not all data at census tract level\"\n",
    "\n",
    "## Create county, census tract, and boro fields\n",
    "dat[\"county\"] = dat[\"entities\"].apply(get_county)\n",
    "#dat[\"census_tract\"] = dat[\"code\"].apply(lambda x: int(x[5:]) if re.match(\"\\d{5}\",x) else None)\n",
    "dat['census_tract'] = dat['code'].apply(lambda x: int(re.sub('36005|36047|36061|36081|36085|36XXX','',str(x))))\n",
    "boros = {\"Kings County\":3,\n",
    "        \"Queens County\":4,\n",
    "        \"Bronx County\":2,\n",
    "        \"New York County\":1,\n",
    "        \"Richmond County\":5}\n",
    "\n",
    "dat[\"borocode\"] = dat[\"county\"].replace(boros)\n",
    "\n",
    "## Create aggregate fields for units and occupied for quality checks\n",
    "dat[\"est_total_occupied\"] = dat[\"total_units\"] * (dat[\"pct_occupied\"] / 100)\n",
    "dat[\"diff_occupied_reported\"] = dat[\"est_total_occupied\"] - dat[\"number_reported\"]\n",
    "\n",
    "## Filter to just NYC\n",
    "cut = dat.loc[dat.county.isin([\n",
    "    \"Kings County\",\n",
    "    \"Queens County\",\n",
    "    \"Bronx County\",\n",
    "    \"Richmond County\",\n",
    "    \"New York County\"\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23d5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to just HCV\n",
    "hcv = cut.loc[cut.program_label == \"Housing Choice Vouchers\",\n",
    "              [\"program_label\",\n",
    "               \"code\",\n",
    "               \"county\",\n",
    "               \"borocode\",\n",
    "               \"census_tract\",\n",
    "               \"number_reported\",\n",
    "               \"people_total\",\n",
    "               \"total_units\",\n",
    "               \"est_total_occupied\",\n",
    "               \"diff_occupied_reported\"]].copy()\n",
    "\n",
    "## Remove suppressed data\n",
    "hcv = hcv.loc[hcv.number_reported > 0]\n",
    "hcv.replace(to_replace = -4, value = None, inplace = True)\n",
    "\n",
    "## Make sure HCV cut is unique on borough and census tract\n",
    "check = hcv.groupby([\"borocode\",\"census_tract\"]).aggregate({\"program_label\":\"count\"})\n",
    "assert len(check.loc[check.program_label > 1]) == 0, \"Error! Data is not unique on borough and census tract\"\n",
    "\n",
    "## Group HCV by borough and census tract\n",
    "hcv = hcv.groupby([\"program_label\",\"borocode\",\"county\",\"census_tract\",\"code\"]).\\\n",
    "    aggregate({\n",
    "        \"total_units\":\"max\",\n",
    "        \"number_reported\":\"max\",\n",
    "        \"est_total_occupied\":\"max\",\n",
    "        \"diff_occupied_reported\":\"max\",\n",
    "        \"people_total\":\"max\"\n",
    "    }).\\\n",
    "    reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad626f",
   "metadata": {},
   "source": [
    "### Check data accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4cbde513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/99/zg43g9jx77n46pfkq365c75r0000gn/T/ipykernel_10622/1650092946.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cut['people_over_reported'] = cut['people_total'] / cut['number_reported']\n",
      "/var/folders/99/zg43g9jx77n46pfkq365c75r0000gn/T/ipykernel_10622/1650092946.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cut['diff_people_over_reported'] = cut['people_over_reported']-cut['people_per_unit']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1166.000000\n",
       "mean       10.831092\n",
       "std         7.247537\n",
       "min        -1.085271\n",
       "25%         6.250000\n",
       "50%         8.320513\n",
       "75%        13.142857\n",
       "max        38.285714\n",
       "dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check that estimated occupied units is not greater than the total number of available units\n",
    "assert len(hcv.loc[hcv.total_units<hcv.est_total_occupied]) == 0, \"Error! Total occupied units greater than total available units\"\n",
    "\n",
    "## Check that the number of reported occupied units is not greater than the total number of available units \n",
    "assert len(hcv.loc[hcv.total_units<hcv.number_reported]) == 0, \"Error! Reported units greater than total available units\"\n",
    "\n",
    "## Check that only 1 record (Bronx 15800) where reported occupied units is less than estimated occupied units\n",
    "assert len(hcv.loc[hcv.diff_occupied_reported<0]) == 1,\"Error! Total occupied units less than reported units\"\n",
    "\n",
    "## Check that dividing people_total by number_reported is an accurate (within 1 decimal place) estimate of hh size\n",
    "cut['people_over_reported'] = cut['people_total'] / cut['number_reported']\n",
    "cut['diff_people_over_reported'] = cut['people_over_reported']-cut['people_per_unit']\n",
    "assert len(cut.loc[(cut.diff_people_over_reported > 0.1) & (cut.people_total > 0),\n",
    "        ['people_total','people_over_reported','people_per_unit']]) == 0, \"Error! people_total/number_reported not an accurate estimate of hh size\"\n",
    "\n",
    "## Check out the average difference between the number of estimated occupied units vs. number reported\n",
    "## Note that there is a pretty big spread (up to 40%)\n",
    "(hcv.diff_occupied_reported / hcv.number_reported*100).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7559f",
   "metadata": {},
   "source": [
    "### Read in census tract shape file and floodplain files, combine with HCV data, join and create overlap boolean\n",
    "\n",
    "**Data Source Definition:** NYC Planning 2010 Census Tract Shape Files\n",
    "\n",
    "**Data source and documenation:** https://www1.nyc.gov/site/planning/data-maps/open-data/census-download-metadata.page\n",
    "\n",
    "**Data Source Definition:** NYC Open Data Portal 100 and 500 year floodplains\n",
    "\n",
    "**Source and documentation**: https://data.cityofnewyork.us/Environment/Sea-Level-Rise-Maps-2020s-100-year-Floodplain-/ezfn-5dsb,\n",
    "https://data.cityofnewyork.us/Environment/Sea-Level-Rise-Maps-2020s-500-year-Floodplain-/ajyu-7sgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc6af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in census tract shapefiles, format census tract code, and group by census tract\n",
    "census10 = gpd.read_file('nyct2010_22a/nyct2010.shp').to_crs(epsg=4326)\n",
    "census10['census_tract'] = census10['CT2010'].apply(lambda x: int(re.sub('^0','',str(x))))\n",
    "census10 = census10[['census_tract','geometry']].drop_duplicates().dissolve(by='census_tract')\n",
    "\n",
    "# Merge to hcv data\n",
    "hcvt = census10.merge(hcv, how = 'right', on = 'census_tract')\n",
    "\n",
    "# Check that merge is complete\n",
    "assert len(hcvt) == len(hcv), \"Error! Merge has changed total number of rows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to geojson to check out and check overlap in QGIS\n",
    "hcvt.to_file(\"archive/hcv_by_tract.geojson\")\n",
    "\n",
    "# Read in floodplain files that have been fixed, clipped to shoreline, and dissolved in QGIS \n",
    "# Many thanks to Joann for advice on this!! https://github.com/joannlee-nyc\n",
    "_500yr_floodplain = gpd.read_file(\"../data/floodplain_500_dissolved.geojson\")\n",
    "_100yr_floodplain = gpd.read_file(\"../data/floodplain_100_dissolved.geojson\")\n",
    "\n",
    "# Rename and format columns\n",
    "join100 = _100yr_floodplain[['geometry','fld_zone','id']].rename(columns={'fld_zone':'fld_zone100','id':'id_100'})\n",
    "join100['id_100'] = join100['id_100'].apply(lambda x: int(x))\n",
    "join500 = _500yr_floodplain[['geometry','fld_zone','id']].rename(columns={'fld_zone':'fld_zone500','id':'id_500'})\n",
    "join500['id_500'] = join500['id_500'].apply(lambda x: int(x))\n",
    "\n",
    "# overlay 500 year floodplain over census tract-level HCV data\n",
    "hcvt_test_500 = hcvt.\\\n",
    "    overlay(join500, how = 'union').\\\n",
    "    drop(columns=['geometry']).\\\n",
    "    groupby(list(hcv.columns.values)).\\\n",
    "    aggregate({'id_500':'max'})\n",
    "\n",
    "# overlay 100 year floodplain over census tract-level HCV data\n",
    "hcvt_test_100 = hcvt.\\\n",
    "    overlay(join100, how = 'union').\\\n",
    "    drop(columns=['geometry']).\\\n",
    "    groupby(list(hcv.columns.values)).\\\n",
    "    aggregate({'id_100':'max'})\n",
    "\n",
    "# create 0/1 variables at census tract level to mark whether the tract is in 100 or 500 year floodplains\n",
    "hcvt_test_500['in_floodplain_500'] = hcvt_test_500.id_500.apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "hcvt_test_100['in_floodplain_100'] = hcvt_test_100.id_100.apply(lambda x: 0 if pd.isna(x) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e6d013",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hcvt_test_100' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/99/zg43g9jx77n46pfkq365c75r0000gn/T/ipykernel_16205/2633672448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# merge 500 and 100 year overlapped files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhcvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhcvt_test_100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhcvt_test_500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhcvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhcvf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_500'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'id_100'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m hcvf['in_either_floodplain'] = hcvf.apply(lambda x: 1 if (x.in_floodplain_500==1) | (x.in_floodplain_100==1) else 0,\n\u001b[1;32m      5\u001b[0m                                          axis = 1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hcvt_test_100' is not defined"
     ]
    }
   ],
   "source": [
    "# merge 500 and 100 year overlapped files\n",
    "hcvf = hcvt_test_100.merge(hcvt_test_500, how = 'inner', left_index = True, right_index = True)\n",
    "hcvf = hcvf.reset_index().drop(columns=['id_500','id_100'])\n",
    "hcvf['in_either_floodplain'] = hcvf.apply(lambda x: 1 if (x.in_floodplain_500==1) | (x.in_floodplain_100==1) else 0,\n",
    "                                         axis = 1)\n",
    "\n",
    "# check that files are identical except for id columns\n",
    "assert len(hcvf)==len(hcvt_test_100), \"Error! Rows lost in merge\"\n",
    "assert len(hcvf)==len(hcvt_test_500), \"Error! Rows lost in merge\"\n",
    "\n",
    "# check that length is the same as original hcv dataset\n",
    "assert len(hcvf)==len(hcvt.loc[hcvt.census_tract!='999999']), \"Error! Changes from original dataset\"\n",
    "\n",
    "hcvf.to_csv(\"archive/hcv_tagged_tract_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fa587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "133361b7",
   "metadata": {},
   "source": [
    "### Get estimated voucher holders in flood plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "548193ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reported households\n",
    "hcvf['hhs_in_100fp'] = hcvf['number_reported'] * hcvf['in_floodplain_100']\n",
    "hcvf['hhs_in_500fp'] = hcvf['number_reported'] * hcvf['in_floodplain_500']\n",
    "hcvf['hhs_in_any_fp'] = hcvf.apply(lambda x: x.number_reported * max(x.in_floodplain_500,x.in_floodplain_100),\n",
    "                                   axis = 1)\n",
    "\n",
    "# Estimated households\n",
    "hcvf['est_hhs_in_100fp'] = hcvf['est_total_occupied'] * hcvf['in_floodplain_100']\n",
    "hcvf['est_hhs_in_500fp'] = hcvf['est_total_occupied'] * hcvf['in_floodplain_500']\n",
    "\n",
    "# Reported people\n",
    "hcvf['people_in_100fp'] = hcvf['people_total'] * hcvf['in_floodplain_100']\n",
    "hcvf['people_in_500fp'] = hcvf['people_total'] * hcvf['in_floodplain_500']\n",
    "\n",
    "# Estimated people\n",
    "hcvf['est_people_in_100fp'] = hcvf['est_total_occupied'] * hcvf['people_total'] / hcvf['number_reported'] * hcvf['in_floodplain_100']\n",
    "hcvf['est_people_in_500fp'] = hcvf['est_total_occupied'] * hcvf['people_total'] / hcvf['number_reported'] * hcvf['in_floodplain_500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "2919b418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>in_floodplain_100</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_floodplain_500</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hhs_in_any_fp</th>\n",
       "      <td>0.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>1361.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "in_floodplain_100    0              1\n",
       "in_floodplain_500    0      1       1\n",
       "hhs_in_any_fp      0.0  489.0  1361.0"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that any floodplain count is accurate\n",
    "hcvf.pivot_table(\n",
    "    columns=['in_floodplain_100','in_floodplain_500'],\n",
    "    values='hhs_in_any_fp',\n",
    "    aggfunc='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77a04f",
   "metadata": {},
   "source": [
    "### Read in PUMA xwalk, filter to NYC, add borough code, and merge to HVC Dataset\n",
    "\n",
    "**Data Source:** https://www2.census.gov/geo/docs/maps-data/data/rel/2010_Census_Tract_to_2010_PUMA.txt\n",
    "\n",
    "**Documentation**: https://www.census.gov/geographies/reference-files/time-series/geo/relationship-files.2010.html#par_list_0\n",
    "\n",
    "**Matching County FP to Borough:** https://guides.newman.baruch.cuny.edu/nyc_data\n",
    "\n",
    "* The Bronx is Bronx County (ANSI / FIPS 36005)\n",
    "* Brooklyn is Kings County (ANSI / FIPS 36047)\n",
    "* Manhattan is New York County (ANSI / FIPS 36061)\n",
    "* Queens is Queens County (ANSI / FIPS 36081)\n",
    "* Staten Island is Richmond County (ANSI / FIPS 36085)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "1793bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in\n",
    "pumas = pd.read_csv(\"2010_Census_Tract_to_2010_PUMA.csv\")\n",
    "\n",
    "## Filter to NY\n",
    "nypumas = pumas.loc[(pumas.STATEFP == 36) &\n",
    "                    (pumas.COUNTYFP.isin([5,47,61,81,85]))].copy()\n",
    "## Add borocode\n",
    "boros = {47:3, 81:4, 5:2, 61:1, 85:5}\n",
    "nypumas[\"borocode\"] = nypumas[\"COUNTYFP\"].replace(boros)\n",
    "\n",
    "# Merge with HCV dataset and check for uniqueness\n",
    "\n",
    "## Merge\n",
    "hcv_puma = hcvf.merge(nypumas, \n",
    "                     how = 'left',\n",
    "                     left_on = ['borocode','census_tract'],\n",
    "                     right_on = ['borocode','TRACTCE'])\n",
    "\n",
    "## Check that all records merged except those with invalid census tract codes\n",
    "assert len(hcv_puma.loc[(hcv_puma.census_tract != 999999) &\n",
    "                        (hcv_puma.TRACTCE.isnull())]) == 0, \"Error! Not all records merged\" \n",
    "\n",
    "## Check that data is unique on borough, tract and puma\n",
    "check = hcv_puma.groupby(['borocode','census_tract','PUMA5CE']).aggregate({'program_label':'count'}).reset_index()\n",
    "assert len(check.loc[check.program_label>1]) == 0, \"Error! Data is not unique on borough, tract, and puma\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd09ded",
   "metadata": {},
   "source": [
    "### Scrape PUMA names from map and aggregate HCV data at PUMA level\n",
    "**Data source:** https://www1.nyc.gov/assets/planning/download/pdf/data-maps/nyc-population/census2010/puma_cd_map.pdf (scraped below)\n",
    "\n",
    "PUMA <> Community districts and PUMA names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dea8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape pdf data into dataframe\n",
    "pdf = \"https://www1.nyc.gov/assets/planning/download/pdf/data-maps/nyc-population/census2010/puma_cd_map.pdf\"\n",
    "df = read_pdf(pdf, pages = 'all')\n",
    "\n",
    "## Parse first three columns\n",
    "table1 = df[0].iloc[:,0:3].dropna(how = \"all\")\n",
    "table1.columns = table1.loc[2]\n",
    "table1 = table1.loc[table1.CD != \"CD\"]\n",
    "\n",
    "## Parse second three columns\n",
    "table2 = df[0].iloc[:,3:5]\n",
    "table2.columns = table2.loc[2]\n",
    "table2[\"CD\"] = table2[\"CD PUMA\"].\\\n",
    "    apply(lambda x: re.findall(\"^(\\d+\\s*\\&*\\d*)\\s\",str(x))[0] if re.match(\"^(\\d+\\s*\\&*\\d*)\\s\",str(x)) else None)\n",
    "table2[\"PUMA\"] = table2[\"CD PUMA\"].\\\n",
    "    apply(lambda x: str(x)[-4:] if re.match(\"\\d{4}\",str(x)[-4:]) else None)\n",
    "table2 = table2[[\"CD\",\"PUMA\",\"PUMA Name\"]].dropna(how = \"all\")\n",
    "\n",
    "## Add Staten Island rows that get cut off\n",
    "table3 = pd.DataFrame(\n",
    "    data = [\n",
    "        [\"1\",\"3903\",\"Port Richmond, Stapleton & Mariner's Harbor\"],\n",
    "        [\"2\",\"3902\",\"New Springville & South Beach\"],\n",
    "        [\"3\",\"3901\",\"Tottenville, Great Kills & Annadale\"]\n",
    "    ],\n",
    "    columns = [\"CD\",\"PUMA\",\"PUMA Name\"])\n",
    "\n",
    "## Combine tables\n",
    "table = pd.concat([table1,table2,table3])\n",
    "table[\"PUMA\"] = table[\"PUMA\"].apply(lambda x: int(x) if x != None else None)\n",
    "\n",
    "## Correct two PUMA names that get wonky\n",
    "table[\"PUMA Name\"] = table[\"PUMA Name\"].replace(\n",
    "    {\n",
    "        \"CD 5Bedford Park, Fordham North & Norwood\":\"Bedford Park, Fordham North & Norwood\",\n",
    "         \"4106Brooklyn Heights & Fort Greene\":\"Brooklyn Heights & Fort Greene\"\n",
    "    })\n",
    "\n",
    "## Check that dataset is unique on PUMA\n",
    "assert (table.PUMA.value_counts() == 1).all() == True, \"Error! PUMA codes that map to more than one PUMA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "399558f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine HCV dataset and PUMA names + CDs, check for uniqueness, aggregate at PUMA level\n",
    "\n",
    "## Merge HCV and PUMA names and CDs\n",
    "hcv_puma_g = hcv_puma.merge(table, how = 'left', left_on = 'PUMA5CE', right_on = 'PUMA')\n",
    "\n",
    "## Check that data is unique on borough, tract and puma\n",
    "check = hcv_puma_g.groupby(['borocode','census_tract','PUMA Name']).\\\n",
    "    aggregate({'program_label':'count'}).\\\n",
    "    reset_index()\n",
    "assert len(check.loc[check.program_label>1]) == 0, \"Error! Data is not unique on borough, tract, and puma\"\n",
    "\n",
    "## Aggregate at PUMA level\n",
    "hcv_puma_g = hcv_puma_g.groupby(['borocode','PUMA','PUMA Name','CD']).\\\n",
    "    aggregate({'est_total_occupied':'sum',\n",
    "               'number_reported':'sum',\n",
    "               'people_total':'sum',\n",
    "               'hhs_in_100fp':'sum',\n",
    "               'hhs_in_500fp':'sum',\n",
    "               'hhs_in_any_fp':'sum',\n",
    "               'est_hhs_in_100fp':'sum',\n",
    "               'est_hhs_in_500fp':'sum',\n",
    "               'people_in_100fp':'sum',\n",
    "               'people_in_500fp':'sum',\n",
    "               'est_people_in_100fp':'sum',\n",
    "               'est_people_in_500fp':'sum',\n",
    "               'hhs_in_any_fp':'sum'\n",
    "               }).\\\n",
    "    reset_index()\n",
    "\n",
    "## Add average voucher household size stat\n",
    "hcv_puma_g['avg_hh_size'] = np.round(hcv_puma_g['people_total']/hcv_puma_g['number_reported'],2)\n",
    "\n",
    "## Add % in floodplain (reported)\n",
    "hcv_puma_g['pct_hh_in_100fp'] = hcv_puma_g['hhs_in_100fp']/hcv_puma_g['number_reported']\n",
    "hcv_puma_g['pct_hh_in_500fp'] = hcv_puma_g['hhs_in_500fp']/hcv_puma_g['number_reported']\n",
    "hcv_puma_g['pct_hh_in_any_fp'] = hcv_puma_g['hhs_in_any_fp']/hcv_puma_g['number_reported']\n",
    "\n",
    "## Check that all records merged\n",
    "assert len(hcv_puma.loc[(~hcv_puma.PUMA5CE.isna()) &\n",
    "                        (~hcv_puma.PUMA5CE.isin(hcv_puma_g.PUMA)),['PUMA5CE']]) == 0, \"Error! PUMAS in HCV dataset are missing in PUMA name dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6afd7e",
   "metadata": {},
   "source": [
    "### Sense check aggregated data--does it line up with expectations about vouchers in NYC?\n",
    "\n",
    "**Data Sources:** https://www1.nyc.gov/site/nycha/section-8/about-section-8.page#:~:text=Approximately%2085%2C000%20Section%208%20vouchers,programs%20in%20New%20York%20City.,\n",
    "https://www1.nyc.gov/site/hpd/services-and-information/about-section-8.page,\n",
    "https://www.cbpp.org/research/housing/federal-rental-assistance-fact-sheets#NY\n",
    "\n",
    "* NYCHA: \"Approximately 85,000 Section 8 vouchers ... currently participate in the program.\"\n",
    "* HPD: \"In total, HPD serives over 39,000 households in all five boroughs.\"\n",
    "* CBPP: \"Number of Households Receiving Major Types of Federal Rental Assistance in New York. Housing Choice Vouchers: 232,000\" for all of New York State\n",
    "\n",
    "Estimated total: 85K + 39K = **124K housholds**, about half the total for New York State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "372406ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reported voucher households: 116,683\n",
      "Total estimated voucher households (% occupied * total units): 129,617\n",
      "Total people in voucher households: 255,140\n",
      "Add data note that total occupied may be overestimating\n"
     ]
    }
   ],
   "source": [
    "print(\"Total reported voucher households: \" + \"{:,.0f}\".format(hcv_puma_g.number_reported.sum()))\n",
    "print(\"Total estimated voucher households (% occupied * total units): \" + \"{:,.0f}\".format(hcv_puma_g.est_total_occupied.sum()))\n",
    "print(\"Total people in voucher households: \" + \"{:,.0f}\".format(hcv_puma_g.people_total.sum()))\n",
    "print(\"Add data note that total occupied may be overestimating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "b5758e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reported voucher households in 100 year floodplain: 53,031\n",
      "Total reported voucher households in 500 year floodplain: 64,497\n",
      "\n",
      "\n",
      "Total estimated voucher households in 100 year floodplain: 59,359\n",
      "Total estimated voucher households in 500 year floodplain: 72,219\n",
      "\n",
      "\n",
      "Total reported people with vouchers in 100 year floodplain: 118,351\n",
      "Total reported people with vouchers in 500 year floodplain: 145,983\n",
      "\n",
      "\n",
      "Total estimated people with vouchers in 100 year floodplain: 131,163\n",
      "Total estimated people with vouchers in 500 year floodplain: 161,814\n",
      "\n",
      "\n",
      "Total reported voucher households in either floodplain: 64,497\n",
      "Percent of reported voucher households in either floodplain: 55%\n"
     ]
    }
   ],
   "source": [
    "print(\"Total reported voucher households in 100 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.hhs_in_100fp.sum()))\n",
    "print(\"Total reported voucher households in 500 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.hhs_in_500fp.sum()))\n",
    "print('\\n')\n",
    "print(\"Total estimated voucher households in 100 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.est_hhs_in_100fp.sum()))\n",
    "print(\"Total estimated voucher households in 500 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.est_hhs_in_500fp.sum()))\n",
    "print('\\n')\n",
    "print(\"Total reported people with vouchers in 100 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.people_in_100fp.sum()))\n",
    "print(\"Total reported people with vouchers in 500 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.people_in_500fp.sum()))\n",
    "print('\\n')\n",
    "print(\"Total estimated people with vouchers in 100 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.est_people_in_100fp.sum()))\n",
    "print(\"Total estimated people with vouchers in 500 year floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.est_people_in_500fp.sum()))\n",
    "print('\\n')\n",
    "print(\"Total reported voucher households in either floodplain: \" + \"{:,.0f}\".format(hcv_puma_g.hhs_in_any_fp.sum()))\n",
    "print(\"Percent of reported voucher households in either floodplain: \" + \"{:.0%}\".format(hcv_puma_g.hhs_in_any_fp.sum()/hcv_puma_g.number_reported.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "04594e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with intersection files prepared manually in QGIS just to be sure\n",
    "intersection_500 = gpd.read_file(\"hcv_floodplain_500_intersection.geojson\")\n",
    "check_500 = intersection_500.\\\n",
    "    groupby(['PUMA','borocode']).\\\n",
    "    aggregate({'number_reported':'sum','est_total_occupied':'sum'}).\\\n",
    "    reset_index()\n",
    "len(hcv_puma_g.merge(check_500, how = 'inner', on = ['PUMA','borocode']))-len(hcv_puma_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "adceb52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borocode</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>PUMA Name</th>\n",
       "      <th>CD</th>\n",
       "      <th>est_total_occupied</th>\n",
       "      <th>number_reported</th>\n",
       "      <th>people_total</th>\n",
       "      <th>hhs_in_100fp</th>\n",
       "      <th>hhs_in_500fp</th>\n",
       "      <th>est_hhs_in_100fp</th>\n",
       "      <th>est_hhs_in_500fp</th>\n",
       "      <th>people_in_100fp</th>\n",
       "      <th>people_in_500fp</th>\n",
       "      <th>est_people_in_100fp</th>\n",
       "      <th>est_people_in_500fp</th>\n",
       "      <th>avg_hh_size</th>\n",
       "      <th>pct_hh_in_100fp</th>\n",
       "      <th>pct_hh_in_500fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3806</td>\n",
       "      <td>Upper West Side &amp; West Side</td>\n",
       "      <td>7</td>\n",
       "      <td>1546.70</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>1916.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>847.82</td>\n",
       "      <td>847.82</td>\n",
       "      <td>890.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>1102.944274</td>\n",
       "      <td>1102.944274</td>\n",
       "      <td>1.49</td>\n",
       "      <td>52.643857</td>\n",
       "      <td>52.643857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3706</td>\n",
       "      <td>Bedford Park, Fordham North &amp; Norwood</td>\n",
       "      <td>7</td>\n",
       "      <td>6735.36</td>\n",
       "      <td>6234.0</td>\n",
       "      <td>11308.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>594.15</td>\n",
       "      <td>594.15</td>\n",
       "      <td>993.0</td>\n",
       "      <td>993.0</td>\n",
       "      <td>1066.891078</td>\n",
       "      <td>1066.891078</td>\n",
       "      <td>1.81</td>\n",
       "      <td>8.870709</td>\n",
       "      <td>8.870709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4015</td>\n",
       "      <td>Flatbush &amp; Midwood</td>\n",
       "      <td>14</td>\n",
       "      <td>2546.56</td>\n",
       "      <td>2349.0</td>\n",
       "      <td>4066.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>215.41</td>\n",
       "      <td>387.31</td>\n",
       "      <td>334.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>341.987484</td>\n",
       "      <td>549.209402</td>\n",
       "      <td>1.73</td>\n",
       "      <td>8.939974</td>\n",
       "      <td>15.155385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    borocode  PUMA                              PUMA Name  CD  \\\n",
       "5        1.0  3806            Upper West Side & West Side   7   \n",
       "15       2.0  3706  Bedford Park, Fordham North & Norwood   7   \n",
       "34       3.0  4015                     Flatbush & Midwood  14   \n",
       "\n",
       "    est_total_occupied  number_reported  people_total  hhs_in_100fp  \\\n",
       "5              1546.70           1286.0        1916.0         677.0   \n",
       "15             6735.36           6234.0       11308.0         553.0   \n",
       "34             2546.56           2349.0        4066.0         210.0   \n",
       "\n",
       "    hhs_in_500fp  est_hhs_in_100fp  est_hhs_in_500fp  people_in_100fp  \\\n",
       "5          677.0            847.82            847.82            890.0   \n",
       "15         553.0            594.15            594.15            993.0   \n",
       "34         356.0            215.41            387.31            334.0   \n",
       "\n",
       "    people_in_500fp  est_people_in_100fp  est_people_in_500fp  avg_hh_size  \\\n",
       "5             890.0          1102.944274          1102.944274         1.49   \n",
       "15            993.0          1066.891078          1066.891078         1.81   \n",
       "34            510.0           341.987484           549.209402         1.73   \n",
       "\n",
       "    pct_hh_in_100fp  pct_hh_in_500fp  \n",
       "5         52.643857        52.643857  \n",
       "15         8.870709         8.870709  \n",
       "34         8.939974        15.155385  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out PUMAs that have 500 year floodplain matches based on geopandas overlay but not in QGIS file\n",
    "hcv_puma_g.loc[~hcv_puma_g.PUMA.isin(check_500.PUMA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "090ee0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_tract</th>\n",
       "      <th>borocode</th>\n",
       "      <th>number_reported</th>\n",
       "      <th>people_in_500fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>15100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>16100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>17100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>17900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>18300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>18700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>18900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>19100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>25500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>26900.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>45600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>46201.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>78800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     census_tract  borocode  number_reported  people_in_500fp\n",
       "50        15100.0       1.0            136.0            281.0\n",
       "54        15700.0       1.0             91.0             95.0\n",
       "56        16100.0       1.0             19.0             19.0\n",
       "63        17100.0       1.0            179.0            181.0\n",
       "70        17900.0       1.0             66.0             71.0\n",
       "74        18300.0       1.0             65.0             70.0\n",
       "76        18700.0       1.0             19.0             52.0\n",
       "78        18900.0       1.0             39.0             58.0\n",
       "80        19100.0       1.0             63.0             63.0\n",
       "325       25500.0       2.0            390.0            699.0\n",
       "336       26900.0       2.0            163.0            294.0\n",
       "700       45600.0       3.0             51.0             73.0\n",
       "702       46201.0       3.0            159.0            261.0\n",
       "814       78800.0       3.0            146.0            176.0"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot check these values\n",
    "# All seem to show up in the \"intersections\" layer in QGIS\n",
    "hcv_puma.loc[(hcv_puma.PUMA5CE.isin([3806,3706,4015])) & \n",
    "             (hcv_puma.people_in_500fp >0),\n",
    "             ['census_tract','borocode','number_reported','people_in_500fp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e86712c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      54.000000\n",
       "mean     2160.796296\n",
       "std      2115.885794\n",
       "min       170.000000\n",
       "25%       476.750000\n",
       "50%      1256.000000\n",
       "75%      3138.500000\n",
       "max      8957.000000\n",
       "Name: number_reported, dtype: float64"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptives for number_reported\n",
    "hcv_puma_g.number_reported.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37673c",
   "metadata": {},
   "source": [
    "### Read in geoJSON datasets, merge with HCV data, write to geoJSON file for mapping\n",
    "\n",
    "**Source and documentation**: https://data.cityofnewyork.us/Housing-Development/2010-Public-Use-Microdata-Areas-PUMAs-/cwiz-gcty\n",
    "\n",
    "**Data Source Definition:** NYC Open Data Portal GeoJSON file for PUMAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "267fa6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>puma</th>\n",
       "      <th>shape_area</th>\n",
       "      <th>shape_leng</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3701</td>\n",
       "      <td>97928516.6801</td>\n",
       "      <td>53227.1136077</td>\n",
       "      <td>MULTIPOLYGON (((-73.89641 40.90450, -73.89636 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3702</td>\n",
       "      <td>188993662.321</td>\n",
       "      <td>106167.716965</td>\n",
       "      <td>MULTIPOLYGON (((-73.86943 40.87813, -73.86950 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3703</td>\n",
       "      <td>267643637.74</td>\n",
       "      <td>305269.139107</td>\n",
       "      <td>MULTIPOLYGON (((-73.78833 40.83467, -73.78931 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3704</td>\n",
       "      <td>106216907.291</td>\n",
       "      <td>47970.2030563</td>\n",
       "      <td>MULTIPOLYGON (((-73.84793 40.87134, -73.84725 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3705</td>\n",
       "      <td>122484260.721</td>\n",
       "      <td>68704.1110155</td>\n",
       "      <td>MULTIPOLYGON (((-73.87046 40.86663, -73.87042 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   puma     shape_area     shape_leng  \\\n",
       "0  3701  97928516.6801  53227.1136077   \n",
       "1  3702  188993662.321  106167.716965   \n",
       "2  3703   267643637.74  305269.139107   \n",
       "3  3704  106216907.291  47970.2030563   \n",
       "4  3705  122484260.721  68704.1110155   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((-73.89641 40.90450, -73.89636 ...  \n",
       "1  MULTIPOLYGON (((-73.86943 40.87813, -73.86950 ...  \n",
       "2  MULTIPOLYGON (((-73.78833 40.83467, -73.78931 ...  \n",
       "3  MULTIPOLYGON (((-73.84793 40.87134, -73.84725 ...  \n",
       "4  MULTIPOLYGON (((-73.87046 40.86663, -73.87042 ...  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in and inspect data\n",
    "puma_json = gpd.read_file(\"https://data.cityofnewyork.us/api/geospatial/cwiz-gcty?method=export&format=GeoJSON\")\n",
    "puma_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert PUMA to string type for merging\n",
    "hcv_puma_g['PUMA'] = hcv_puma_g['PUMA'].apply(lambda x: str(int(x)))\n",
    "\n",
    "## Merge \n",
    "viz = puma_json.merge(hcv_puma_g,\n",
    "               how = 'inner',\n",
    "               left_on = 'puma',\n",
    "               right_on = 'PUMA')\n",
    "\n",
    "## Check that all records merged\n",
    "assert len(hcv_puma_g.loc[~(hcv_puma_g.PUMA.isin(viz.PUMA))]) == 0, \"Error! Not all records merged to geoJSON\"\n",
    "\n",
    "## rename PUMA Name to proper variable name\n",
    "viz.rename(columns = {\"PUMA Name\":\"puma_name\"}, inplace = True)\n",
    "\n",
    "## Write to geoJSON file for reduction on https://mapshaper.org/ and then mapping\n",
    "viz.to_file(\"../data/hcv_dat.geojson\",driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
